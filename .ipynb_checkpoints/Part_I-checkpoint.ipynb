{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up deterministic flag\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds\n",
    "SEED = 666\n",
    "tf.random.set_seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wandb\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up and preprocess data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify shapes\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the class labels\n",
    "LABELS = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\n",
    "        \"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sepcific validation set from the test set\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "idx = np.random.choice(test_images.shape[0], 32)\n",
    "for i in idx:\n",
    "    X_val.append(test_images[i])\n",
    "    y_val.append(test_labels[i])\n",
    "    \n",
    "X_val, y_val = np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 28, 28), (32,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify shapes\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary\n",
    "config_defaults = {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"prefinal_activation\": \"relu\",\n",
    "        \"final_activation\": \"softmax\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        'seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/sayakpaul/reproducible-ml\" target=\"_blank\">https://app.wandb.ai/sayakpaul/reproducible-ml</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/sayakpaul/reproducible-ml/runs/no-sweeps\" target=\"_blank\">https://app.wandb.ai/sayakpaul/reproducible-ml/runs/no-sweeps</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generating diff: Reference at 'refs/remotes/origin/master' does not exist\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"reproducible-ml\", id=\"no-sweeps\", config=config_defaults)\n",
    "\n",
    "# Config is a variable that holds and saves hyperparameters and inputs\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                      patience=1,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for model\n",
    "def get_training_model():\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(256, activation=config.prefinal_activation),\n",
    "        tf.keras.layers.Dense(10, activation=config.final_activation)\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=config.optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generating diff: Reference at 'refs/remotes/origin/master' does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.5299 - accuracy: 0.8147 - val_loss: 0.4510 - val_accuracy: 0.8429\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3899 - accuracy: 0.8612 - val_loss: 0.4069 - val_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3474 - accuracy: 0.8751 - val_loss: 0.3722 - val_accuracy: 0.8629\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3189 - accuracy: 0.8850 - val_loss: 0.3599 - val_accuracy: 0.8709\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3039 - accuracy: 0.8880 - val_loss: 0.3498 - val_accuracy: 0.8755\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2871 - accuracy: 0.8940 - val_loss: 0.3409 - val_accuracy: 0.8783\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2706 - accuracy: 0.9003 - val_loss: 0.3250 - val_accuracy: 0.8864\n",
      "Epoch 8/10\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.2618 - accuracy: 0.9036Restoring model weights from the end of the best epoch.\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2618 - accuracy: 0.9035 - val_loss: 0.3449 - val_accuracy: 0.8744\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start = time.time()\n",
    "mlp_model = get_training_model()\n",
    "mlp_model.fit(train_images, train_labels, \n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=[WandbCallback(data_type=\"image\", validation_data=(X_val, y_val), \n",
    "                    labels=LABELS),\n",
    "              es])\n",
    "wandb.log({\"training_time\":time.time()-start})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running run number  1\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  2\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  3\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Running run number  4\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  5\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  6\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  7\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Running run number  8\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  9\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  10\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  11\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Running run number  12\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Running run number  13\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Running run number  14\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Running run number  15\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Mean val accuracy 0.8763200640678406 mean val loss 0.3461266803105672\n",
      "Std val accuracy 0.0033160645980387926 Std val loss 0.0053014456736163\n",
      "Mean time to train 10.15922474861145\n",
      "Std time to train 1.0823407448161604\n"
     ]
    }
   ],
   "source": [
    "# Running the model a few times locally\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "times = []\n",
    "\n",
    "for i in range(15):\n",
    "    print(\"Running run number \",i+1)\n",
    "    start = time.time()\n",
    "    mlp_model = get_training_model()\n",
    "    history = mlp_model.fit(train_images, train_labels, \n",
    "        validation_data=(test_images, test_labels),\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epochs,\n",
    "        callbacks=[es],\n",
    "        verbose=0)\n",
    "    end = time.time() - start\n",
    "    \n",
    "    val_accuracy = history.history[\"val_accuracy\"][-1]\n",
    "    val_loss = history.history[\"val_loss\"][-1]\n",
    "    \n",
    "    accuracy_list.append(val_accuracy)\n",
    "    loss_list.append(val_loss)\n",
    "    times.append(end)\n",
    "    \n",
    "print(\"Mean val accuracy {} mean val loss {}\".format(np.mean(accuracy_list), np.mean(loss_list)))\n",
    "print(\"Std val accuracy {} Std val loss {}\".format(np.std(accuracy_list), np.std(loss_list)))\n",
    "print(\"Mean time to train {}\".format(np.mean(times)))\n",
    "print(\"Std time to train {}\".format(np.std(times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the standard deviatiosn of the validation accuracy and validation loss are pretty low. So, we are good here. You might thnik that the dataset and the model are too low to conclude anything but note that as you would scale up these concepts would still apply there :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
